<!DOCTYPE html>
<html lang="en-gb">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Data Engineering | Agent Arachnids Anthology</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Storyplaces I was employed as a placement with my University. I was given a goal to present my analysis findings to a group of PhD students, and to help me with this my manager gave me a list of tasks that needed doing from the weekly level, with an individual list of goals for each week. I used primarily Python and more specifically Pandas for this project.
For the first week it was purely exploratory analysis, finding basic things out like how many users on any given day, how long did each user spend on average, etc.">
    <meta name="generator" content="Hugo 0.133.0">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Paul Brown">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/university-projects/data-engineering/">
    

    <meta property="og:url" content="http://localhost:1313/university-projects/data-engineering/">
  <meta property="og:site_name" content="Agent Arachnids Anthology">
  <meta property="og:title" content="Data Engineering">
  <meta property="og:description" content="Storyplaces I was employed as a placement with my University. I was given a goal to present my analysis findings to a group of PhD students, and to help me with this my manager gave me a list of tasks that needed doing from the weekly level, with an individual list of goals for each week. I used primarily Python and more specifically Pandas for this project.
For the first week it was purely exploratory analysis, finding basic things out like how many users on any given day, how long did each user spend on average, etc.">
  <meta property="og:locale" content="en_gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="university-projects">
    <meta property="article:published_time" content="2023-09-05T11:46:29+01:00">
    <meta property="article:modified_time" content="2023-09-05T11:46:29+01:00">

  <meta itemprop="name" content="Data Engineering">
  <meta itemprop="description" content="Storyplaces I was employed as a placement with my University. I was given a goal to present my analysis findings to a group of PhD students, and to help me with this my manager gave me a list of tasks that needed doing from the weekly level, with an individual list of goals for each week. I used primarily Python and more specifically Pandas for this project.
For the first week it was purely exploratory analysis, finding basic things out like how many users on any given day, how long did each user spend on average, etc.">
  <meta itemprop="datePublished" content="2023-09-05T11:46:29+01:00">
  <meta itemprop="dateModified" content="2023-09-05T11:46:29+01:00">
  <meta itemprop="wordCount" content="465">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Data Engineering">
  <meta name="twitter:description" content="Storyplaces I was employed as a placement with my University. I was given a goal to present my analysis findings to a group of PhD students, and to help me with this my manager gave me a list of tasks that needed doing from the weekly level, with an individual list of goals for each week. I used primarily Python and more specifically Pandas for this project.
For the first week it was purely exploratory analysis, finding basic things out like how many users on any given day, how long did each user spend on average, etc.">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Agent Arachnids Anthology
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/personal-projects/" title="Personal Projects page">
              Personal Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/professional-projects/" title="Professional Projects page">
              Professional Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/tutorials/" title="Tutorials page">
              Tutorials
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/university-projects/" title="University Projects page">
              University Projects
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        University Projects
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Data Engineering</h1>
      
      <p class="tracked">
        By <strong>Paul Brown</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2023-09-05T11:46:29+01:00">September 5, 2023</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="storyplaces">Storyplaces</h2>
<p>I was employed as a placement with my University. I was given a goal to present my analysis findings to a group of PhD students, and to help me with this my manager gave me a list of tasks that needed doing from the weekly level, with an individual list of goals for each week. I used primarily Python and more specifically Pandas for this project.</p>
<p>For the first week it was purely exploratory analysis, finding basic things out like how many users on any given day, how long did each user spend on average, etc.</p>
<p>The second week was focused on more in depth analysis, more specifically user classification. Firstly, I had to come up with an algorithm that would take in an individual&rsquo;s metrics, calculated by crossreferencing the analysis I did in the first week, and designate a classification.</p>
<p>The third week was centred around choice analysis and pattern recognition, like if 2 pages were far apart if that effected player&rsquo;s choice or if a certain path is popular and why? I created a graphical representation of the story, and using this I created a list of every possible path in the story in varying lengths between the distance between 2 nodes and from the top and bottom of the graph. Then converting these paths to a string that could be looked up in each player&rsquo;s path, I could easily get the frequency of each path.</p>
<p>The final week was preparing for the presentation, so I was creating visualisations for the data. I used R Studio &amp; Excel to create the geographical visualisations and Python for the other graphs.</p>
<h2 id="conference-paper">Conference Paper</h2>
<p>I was recommended to do some data engineering for another professor at Bournemouth University whilst I was in my final semester and the <strong><a href="https://www.researchgate.net/publication/376789395_Association_between_air_temperature_and_unintentional_drowning_risk_in_the_United_Kingdom_2012-2019_A_nationwide_case_crossover_study">paper</a></strong> was published. I was given a list of data files and was told that I had to find the coordinates of each incident in the file and the closest station associated with the station. This would essentially find which station is responsible for each drowning incident. The simplest way of determining the distance was using the Haversine Formula. This alone provided some interesting challenges as the data showed which stations were permanently closed down before the incident and some were still open but the incident happened outside of their normal operational hours, so my method getting around this was if one station was invalid, then it would recursively search for the next nearest one. It would do this until either a valid station is found or the search limit is reached, I set this to 10 by default. My final task was to add the Air Temperature and Rainfall metrics from the previous 14 days from the incident date to the data set and then export it.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Agent Arachnids Anthology 2025 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
